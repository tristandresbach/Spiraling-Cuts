{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date,timedelta \n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import mysql.connector\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "server = 'bieno-da-d-80166-unilevercom-sql-01.database.windows.net'\n",
    "database = 'bieno-da-d-80166-unilevercom-sqldb-01'\n",
    "username = 'PROD_LoadPool'\n",
    "password = 'Dlk45^8&&dodsslk12Ad7*as'\n",
    "driver = 'SQL Server Native Client 11.0'\n",
    "port = '1433'\n",
    "pd.options.mode.chained_assignment = None\n",
    "data_directory = 'C:/Users/tristan.dresbach/OneDrive - Unilever/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get fullfilled orders:\n",
    "sql ='''\n",
    "select MaterialNumber,SalesOrderCode,CustomerSoldTo,ReportingDate, QuantityExpected,QuantityDispatched from RawReporting_BDL.FactSalesOrderLine\n",
    "where ReportingDate >= '2021-03-01' and ReportingDate < '2021-03-28' and QuantityOrdered = QuantityDispatched and QuantityOrdered <> 0 and SalesOrganisationCode in ('0001')\n",
    "'''\n",
    "#EDIT BACK SALES ORGS: '0002','0003','0007'\n",
    "connection_string = 'DRIVER={};SERVER={};PORT={};DATABASE={};UID={};PWD={}'.format(\n",
    "    driver, server, port, database, username, password)\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "good_orders = pd.read_sql(sql, cnxn)  # Can be assigned to any variable name\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final customer expected order qty\n",
    "# if we want to run the code for a specific list of input DUs given to us by the business, here: \"Book1.xlsx\",\n",
    "#we filter our data for those items\n",
    "#tgt_items = pd.read_excel('Book1.xlsx')\n",
    "#tgt_items['DU'] = [(18-len(str(x)))*'0' + str(x) for x in tgt_items['DU']]\n",
    "#good_orders = good_orders[good_orders['MaterialNumber'].isin(tgt_items['DU'].tolist())]\n",
    "#good_orders = good_orders.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our cuts column to be used later on\n",
    "good_orders['LBD_QT'] = good_orders['QuantityExpected'] - good_orders['QuantityDispatched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get cut orders:\n",
    "#the cut column is \"(LI.QuantityLossBeforeDispatch*-1) [LBD_QT]\", we use the same date range as for the previous \"good_orders\" SQL query.\n",
    "# last line of SQL query: \"AND LI.QuantityLossBeforeDispatch <> 0\" guarantees that we are only pulling cuts\n",
    "\n",
    "sql ='''\n",
    "    SELECT\n",
    "            LI.MaterialNumber\n",
    "            , LI.[SalesOrderCode]\n",
    "            , LI.CustomerSoldTo\n",
    "            , LI.ReportingDate\n",
    "            , LI.QuantityExpected\n",
    "            ,LI.QuantityDispatched\n",
    "\n",
    "           \n",
    "           ,(LI.QuantityLossBeforeDispatch*-1) [LBD_QT]\n",
    "           \n",
    "        FROM [RawReporting_BDL].[FactSalesOrderLine] LI\n",
    "        INNER JOIN [TeradataConnect].[DimCalendar] CAL ON LI.ReportingDate = CAL.Date_Dt\n",
    "        INNER JOIN [RawReporting_BDL].[DimSalesOrganisation] AS SO ON LI.SalesOrganisationCode =SO.SalesOrganisationCode\n",
    "        INNER JOIN [RawReporting_BDL].[DimDistributionChannel] AS DC ON LI.DistributionChannelCode =DC.DistributionChannelCode\n",
    "        INNER JOIN [RawReporting_BDL].[HierarchyCordilleraLocalProduct] AS PS ON LI.MaterialNumber =PS.MaterialNumber AND LI.SalesOrganisationCode=PS.SalesOrganisationCode AND LI.DistributionChannelCode=PS.DistributionChannel\n",
    "        WHERE CAL.Days_From_Current > 0 and CAL.Date_Dt >= '2021-03-01' and CAL.Date_dt < '2021-03-28'\n",
    "        AND LI.DistributionChannelCode IN ('10','61') ---different distribution channels, \n",
    "        AND LI.SalesOrganisationCode IN ('0001')\n",
    "        AND LI.UnitOfMeasureCode = 'CS'\n",
    "        AND PS.LocalLevel3Code NOT IN ( '006', '0FF', '008', '04D', '0F1')\n",
    "        AND LI.QuantityLossBeforeDispatch <> 0\n",
    "'''\n",
    "\n",
    "#'0002','0003','0007'\n",
    "connection_string = 'DRIVER={};SERVER={};PORT={};DATABASE={};UID={};PWD={}'.format(\n",
    "    driver, server, port, database, username, password)\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "orders_cut = pd.read_sql(sql, cnxn)  # Can be assigned to any variable name\n",
    "cnxn.close()\n",
    "#AND LI.QuantityLossBeforeDispatch <> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these two lines below are part of the same process to filter the data for the items in which we are interested from that input file\n",
    "\n",
    "#orders_cut = orders_cut[orders_cut['MaterialNumber'].isin(tgt_items['DU'].tolist())]\n",
    "#orders_cut = orders_cut.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get our order df which will serves as our data base\n",
    "orders = pd.concat([good_orders,orders_cut],ignore_index=True)\n",
    "orders.rename(columns={'LBD_QT':'Cuts','ReportingDate':'date'},inplace=True)\n",
    "list_DUs = list(set(orders['MaterialNumber']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of sold tos, for names:\n",
    "customers = list(set(orders['CustomerSoldTo']))\n",
    "customers_str = str(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all customer names, using the list of soldtos we just generated from our data:\n",
    "sql ='''\n",
    "select CustomerCode,EccCustomerLevel5Name from RawReporting_BDL.HierarchyCordilleraCustomer\n",
    "where CustomerCode in ''' + customers_str.replace('[','(').replace(']',')')\n",
    "connection_string = 'DRIVER={};SERVER={};PORT={};DATABASE={};UID={};PWD={}'.format(\n",
    "    driver, server, port, database, username, password)\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "cus_names = pd.read_sql(sql, cnxn)  # Can be assigned to any variable name\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append customer names and merge our base df (\"orders\") with the customer related data:\n",
    "cus_names.rename(columns={'CustomerCode':'CustomerSoldTo','EccCustomerLevel5Name':'Customer'},inplace=True)\n",
    "cus_names = cus_names.drop_duplicates(subset=['CustomerSoldTo'],keep='first').reset_index(drop=True)\n",
    "df = pd.merge(orders,cus_names,how='left',on='CustomerSoldTo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an example of subsetting for a specific customer: Walmart only\n",
    "#df = df[df['Customer'] == 'WAL-MART']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep df for analysis:\n",
    "#we filter out weird data issues like DUs that have empty spaces, weird amount of characters, are = -1\n",
    "#create key column for future looping as well\n",
    "df_f = df.copy()\n",
    "#df_f = df_f.sort_values(['MaterialNumber','date']).reset_index(drop=True)\n",
    "df_f = df_f[~df_f['MaterialNumber'].str.contains(' ')].reset_index(drop=True)\n",
    "df_f = df_f[~df_f['MaterialNumber'].str.len() < 5].reset_index(drop=True)\n",
    "#df_f = df_f[df_f['date'] <= date.today()]\n",
    "#df_f = df_f[(df_f['QuantityDispatched'] >= 0) & (df_f['QuantityOriginal'] > 0)].reset_index(drop=True)\n",
    "df_f = df_f[df_f['MaterialNumber'] != '-1']\n",
    "df_f['key'] = df_f['MaterialNumber'] + '_' + df_f['CustomerSoldTo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup output:\n",
    "#want to view the data at a DU & lane level hence we do a groupby to know how much should have shipped and wasn't\n",
    "#we create a list of keys from this that will be the DU/lane we are interested in looping through to identify true demand\n",
    "\n",
    "output_df = df_f.groupby(['MaterialNumber','CustomerSoldTo','Customer','key'],as_index=False)[['QuantityExpected','Cuts']].sum()\n",
    "output_df.drop_duplicates(inplace=True)\n",
    "output_df = output_df[output_df['Cuts'] > 1]\n",
    "output_df = output_df[output_df['QuantityExpected'] > 1]\n",
    "output_df = output_df.sort_values(['Cuts'],ascending=False)\n",
    "output_df = output_df.reset_index(drop=True)\n",
    "df_f_m = df_f[df_f['key'].isin(list(output_df['key']))]\n",
    "df_f_m = df_f_m.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e864771dbc7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#iterate through each key:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'key'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#to get an idea of timing, prints the % completed as it goes through\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_output_nocap\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m300\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_output_nocap = []\n",
    "key_output_nocap = []\n",
    "date_output_nocap = []\n",
    "avg_dates_nocap = []\n",
    "ratios = []\n",
    "days_left_t = []\n",
    "next_p_t = []\n",
    "\n",
    "#iterate through each key:\n",
    "for key in list(output_df['key']):\n",
    "    #to get an idea of timing, prints the % completed as it goes through\n",
    "    if len(key_output_nocap) % 300 == 0:\n",
    "        print('% done :',round(len(key_output_nocap)/len(output_df),2))\n",
    "    #subset the df for POs we are specifically interested in:\n",
    "    df = df_f_m[df_f_m['key'] == key]\n",
    "    df = df[df['QuantityExpected'] > 0]\n",
    "    cuts_t = []\n",
    "    df = df.sort_values(by=['date'])\n",
    "    dates_l = sorted(set(df['date']))\n",
    "    ordered_t= []\n",
    "    cuts_q = []\n",
    "    ordered_q = []\n",
    "    leng = (date.today() - min(list(df['date']))).days\n",
    "    #create our base lists of day differences, cuts and orders\n",
    "    full_cuts = []\n",
    "    for i in range(leng+1):\n",
    "        day =  date.today() - timedelta(days=i)\n",
    "        cut = sum(list(df[df['date'] == day]['Cuts']))\n",
    "        ord_ = sum(list(df[df['date'] == day]['QuantityExpected']))\n",
    "        #print(day,ord_)\n",
    "        if ord_ > 0.1:\n",
    "            cuts_q += [cut]\n",
    "            ordered_q += [ord_]\n",
    "        full_cuts += [cut]\n",
    "        cuts_t+= [cut]\n",
    "        ordered_t += [ord_]\n",
    "    #re verse the lists so they are in chrono-logical order\n",
    "    full_cuts = full_cuts[::-1]\n",
    "    cuts_t = cuts_t[::-1]\n",
    "    ordered_t = ordered_t[::-1]\n",
    "    cuts_q = cuts_q[::-1]\n",
    "    ordered_q = ordered_q[::-1]\n",
    "    #get the average orders and cuts:\n",
    "    avg_order = np.mean([x for x in cuts_q if x > 1])\n",
    "    avg_cuts = np.mean([x for x in ordered_q if x > 1])\n",
    "    \n",
    "    count_indx = []\n",
    "    ct = 0\n",
    "    for x in ordered_t:\n",
    "        if x > 0.1:\n",
    "            count_indx += [ct]\n",
    "        ct +=1\n",
    "    \n",
    "    #initialize variables in the below loop:\n",
    "    i = 0\n",
    "    loop = True\n",
    "    cumu = 0\n",
    "    indx_cumu = []\n",
    "    temp_loss = 0\n",
    "    temp_dates = []\n",
    "    date_avgs = []\n",
    "    loss_list = []\n",
    "    ordered_list = []\n",
    "    drop_indxs = []\n",
    "    full_indxs = []\n",
    "    full_loss = []\n",
    "    min_date = min(list(df['date']))\n",
    "    \n",
    "    #we first check if there are cuts, and if so get a sum for each subsequent cut quantity, as a roling sum:\n",
    "    while loop == True:\n",
    "        temp_date = []\n",
    "        temp_avg = []\n",
    "        cc1 = 0\n",
    "        cc2 = 0\n",
    "        cc3 = 0\n",
    "        loss = 0\n",
    "        if i+2 < len(cuts_q):\n",
    "            cc1 += cuts_q[i] + cuts_q[i+1]\n",
    "        if i+3 < len(cuts_q):\n",
    "            cc2 += cc1 + cuts_q[i+2]\n",
    "        if i+4 < len(cuts_q):\n",
    "            cc3 += cc2 + cuts_q[i+3]\n",
    "        \n",
    "        #use these subsequent cuts to test if: we have an increase in cuts, if the next cut is above average cuts and if the next order is above 0.3* average order and if the sum of cuts is above the average cuts.\n",
    "        #we also check to make sure all these cuts/orders happen within the window of 11 days:\n",
    "        \n",
    "        if (i+1 < len(cuts_q)) and (cuts_q[i] < cuts_q[i+1]) and (cuts_q[i+1]>=1.0*avg_cuts) and (ordered_q[i+1] >= 0.3*avg_order) and (cuts_q[i] >= 0.1*avg_cuts) and ( (count_indx[i+1] - count_indx[i]) < 11) :\n",
    "            temp_loss += cuts_q[i+1] + cuts_q[i]\n",
    "            temp_date = [min_date + timedelta(days = count_indx[i]),min_date + timedelta(days = count_indx[i+1])]\n",
    "            #print(cuts_q[i+1]-cuts_q[i],'  c1') \n",
    "            i += 1\n",
    "            \n",
    "        elif  ((i+2 < len(cuts_q)) and (cc1 < cuts_q[i+2]) and (cuts_q[i+2]>=1.0*avg_cuts) and (ordered_q[i+2] >= 0.3*avg_order) and (cc1 >= 0.1*avg_cuts) and  (count_indx[i+2] - count_indx[i] < 11)):\n",
    "            #print(cuts_q[i+2] - cc1,'  c2')\n",
    "            temp_loss += cuts_q[i+2] + cc1\n",
    "            temp_date = [min_date + timedelta(days = count_indx[i]),min_date + timedelta(days = count_indx[i+1]),min_date + timedelta(days = count_indx[i+2])]\n",
    "            i += 2\n",
    "            \n",
    "        elif  ((i+3 < len(cuts_q)) and (cc2 < cuts_q[i+3]) and (cuts_q[i+3]>=1.0*avg_cuts) and (ordered_q[i+3] >= 0.3*avg_order) and (cc2 >= 0.1*avg_cuts) and (count_indx[i+3] - count_indx[i] < 11)):\n",
    "            #print(cuts_q[i+3] - cc2,'  c2')\n",
    "            temp_loss += cuts_q[i+3] + cc2\n",
    "            temp_date = [min_date + timedelta(days = count_indx[i]),min_date + timedelta(days = count_indx[i+1]),min_date + timedelta(days = count_indx[i+2]),min_date + timedelta(days = count_indx[i+3])]\n",
    "            i += 3\n",
    "\n",
    "        elif  ((i+4 < len(cuts_q)) and (cc3 < cuts_q[i+4]) and (cuts_q[i+4]>=1.0*avg_cuts) and (ordered_q[i+4] >= 0.3*avg_order) and (cc3 >= 0.1*avg_cuts) and (count_indx[i+4] - count_indx[i] < 11)):\n",
    "            #print(cuts_q[i+4] - cc3,'  c3')\n",
    "            temp_loss += cuts_q[i+4] + cc3\n",
    "            temp_date = [min_date + timedelta(days = count_indx[i]),min_date + timedelta(days = count_indx[i+1]),min_date + timedelta(days = count_indx[i+2]),min_date + timedelta(days = count_indx[i+3]),min_date + timedelta(days = count_indx[i+4])]\n",
    "            i += 4\n",
    "\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "        if i > len(cuts_q):\n",
    "            loop = False\n",
    "        #for the above patterns that have been flagged as True, calcualte the amount of losses (for orders and cuts) that took place and what time frame it was on\n",
    "        if len(temp_date) != 0:\n",
    "            temp_dates += temp_date\n",
    "            avg_date = min(temp_date) + timedelta(days = round((max(temp_date) - min(temp_date)).days/2,0))\n",
    "            date_avgs += [avg_date]\n",
    "            #setup lists:\n",
    "            indxs = []\n",
    "            for day in temp_date:\n",
    "                indxs += [dates_l.index(day)]\n",
    "            ordered_loss = []\n",
    "            ordered_q_sum = []\n",
    "            drop_indxs += indxs\n",
    "            full_indxs += [indxs] \n",
    "            for i in indxs:\n",
    "                ordered_loss += [cuts_q[i]]\n",
    "                ordered_q_sum += [ordered_q[i]]\n",
    "            full_loss += [ordered_loss]\n",
    "            loss_list +=[sum(ordered_loss)]\n",
    "            ordered_list += [sum(ordered_q_sum)]\n",
    "    \n",
    "    #this adjusts the losses over time, want to reflect the importance of recent losses vs older losses\n",
    "    if temp_loss != 0:\n",
    "        cuts_base = [i for j, i in enumerate(cuts_q) if j not in drop_indxs]\n",
    "        dates_base = [i for j, i in enumerate(dates_l) if j not in drop_indxs]\n",
    "        #print(loss_list,ordered_list,full_indxs)\n",
    "        ordered_study =  ordered_q\n",
    "        loss_study = [cuts_q[i] if i in drop_indxs else 0 for i in range(len(ordered_study))]\n",
    "        ordered_adj = ordered_study\n",
    "        loss_adj = loss_study\n",
    "        \n",
    "        adj_coeffs = [round(math.exp(-i/len(loss_adj)),2) for i in range(len(loss_adj))][::-1]\n",
    "        adj_coeffs = [round(x*x,2) for x in adj_coeffs]\n",
    "        ordered_adj_coef = [round(x*y,2) for x,y in zip(ordered_adj,adj_coeffs)]\n",
    "        loss_adj_coef = [round(x*y,2) for x,y in zip(loss_adj,adj_coeffs)]\n",
    "        golden_ratio = round(sum(loss_adj_coef)/sum(ordered_adj_coef),2)\n",
    "        ratios += [golden_ratio]\n",
    "        \n",
    "        date_list = [min(dates_l) + datetime.timedelta(days=i) for i in range(leng+1)]\n",
    "\n",
    "        ratios += ['no loss']\n",
    "            \n",
    "    \n",
    "    key_output_nocap += [key]\n",
    "    date_output_nocap += [temp_dates]\n",
    "    avg_dates_nocap += [date_avgs]\n",
    "    \n",
    "    #remove duplicate counts:\n",
    "    days_sub = {}\n",
    "    for day in list(set(temp_dates)):\n",
    "        ct_ = temp_dates.count(day)\n",
    "        if ct_ > 1:\n",
    "            temp_loss -= (ct_-1) * list(df[df['date'] == day]['Cuts'])[0]\n",
    "        \n",
    "    loss_output_nocap += [temp_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data for allocation script:\n",
    "ratios_df = pd.DataFrame()\n",
    "data_directory = 'C:/Users/tristan.dresbach/OneDrive - Unilever/CC_orders/'\n",
    "ratios_df['key'] = key_output_nocap[0:len(key_output_nocap)]\n",
    "ratios_df['loss'] = loss_output_nocap[0:len(key_output_nocap)]\n",
    "ratios_df['date'] = date_output_nocap[0:len(key_output_nocap)]\n",
    "ratios_df['ratio'] = ratios[0:len(key_output_nocap)]\n",
    "outpath = data_directory + 'ratio_data.xlsx'\n",
    "ratios_df.to_excel(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in-depth output for the business:\n",
    "\n",
    "\n",
    "#create our data frame with the data from the previous loops\n",
    "no_cap = pd.DataFrame()\n",
    "no_cap['key'] = key_output_nocap[0:len(key_output_nocap)]\n",
    "no_cap['loss'] = loss_output_nocap[0:len(key_output_nocap)]\n",
    "no_cap['date'] = date_output_nocap[0:len(key_output_nocap)]\n",
    "no_cap['ratio'] = ratios[0:len(key_output_nocap)]\n",
    "\n",
    "final_nocap = pd.merge(output_df,no_cap,how='left',on='key')\n",
    "\n",
    "final_nocap = final_nocap.fillna('not ran')\n",
    "\n",
    "study_nocap = final_nocap.copy()\n",
    "\n",
    "#the below is just getting data the business likes to see from SQL and diong a bunch of merges\n",
    "#get DU rankings:\n",
    "sql ='''\n",
    "select MaterialNumber,ABCD_NSV as priority from Segmentation.MasterAbcdByDu\n",
    "where MaterialNumber in ''' + str(list(study_nocap['MaterialNumber'])).replace('[','(').replace(']',')')\n",
    "connection_string = 'DRIVER={};SERVER={};PORT={};DATABASE={};UID={};PWD={}'.format(\n",
    "    driver, server, port, database, username, password)\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "prio = pd.read_sql(sql, cnxn)  # Can be assigned to any variable name\n",
    "cnxn.close()\n",
    "\n",
    "study_nocap = pd.merge(study_nocap,prio,how='left',on='MaterialNumber')\n",
    "\n",
    "#get item names:\n",
    "sql ='''\n",
    "select MaterialNumber,MaterialDescription as Description from RawReporting_BDL.DimMaterial\n",
    "where MaterialNumber in ''' + str(list(study_nocap['MaterialNumber'])).replace('[','(').replace(']',')')\n",
    "connection_string = 'DRIVER={};SERVER={};PORT={};DATABASE={};UID={};PWD={}'.format(\n",
    "    driver, server, port, database, username, password)\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "mat_desc = pd.read_sql(sql, cnxn)  # Can be assigned to any variable name\n",
    "cnxn.close()\n",
    "\n",
    "mat_desc = mat_desc.drop_duplicates(subset=['MaterialNumber'], keep='first').reset_index(drop=True)\n",
    "\n",
    "study_nocap = pd.merge(study_nocap,mat_desc,how='left',on='MaterialNumber')\n",
    "\n",
    "\n",
    "#get description and brand:\n",
    "sql = ('''\n",
    "select A.MaterialNumber,APOBrandName,CategoryName,DivisionName from RawReporting_BDL.HierarchyCordilleraCategory A\n",
    "left join (select MaterialNumber,MaterialDescription,APOBrandName from RawReporting_BDL.HierarchyAPOCordilleraProduct) B\n",
    "on A.MaterialNumber = B.MaterialNumber\n",
    "where A.MaterialNumber in ''' + str(list(study_nocap['MaterialNumber'])).replace('[','(').replace(']',')')\n",
    ")\n",
    "\n",
    "connection_string = 'DRIVER={};SERVER={};PORT={};DATABASE={};UID={};PWD={}'.format(\n",
    "    driver, server, port, database, username, password)\n",
    "cnxn = pyodbc.connect(connection_string)\n",
    "ref_df = pd.read_sql(sql, cnxn)  # Can be assigned to any variable name\n",
    "ref_df = ref_df.drop_duplicates(subset=['MaterialNumber'], keep='first').reset_index(drop=True)\n",
    "\n",
    "\n",
    "study_nocap = pd.merge(study_nocap,ref_df,how='left',on='MaterialNumber')\n",
    "\n",
    "\n",
    "\n",
    "by_items_nc = study_nocap.groupby(['MaterialNumber','Description','APOBrandName','CategoryName','DivisionName','priority'],as_index=False)[['loss','QuantityExpected','Cuts']].sum()\n",
    "by_items_nc = by_items_nc.sort_values(['loss'],ascending = False)\n",
    "by_items_nc = by_items_nc.reset_index(drop=True)\n",
    "\n",
    "\n",
    "by_customer_nc = study_nocap.groupby(['Customer'],as_index=False)[['loss','QuantityExpected','Cuts']].sum()\n",
    "by_customer_nc = by_customer_nc.sort_values(['loss'],ascending=False)\n",
    "by_customer_nc = by_customer_nc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tgt = study_nocap[study_nocap['Customer'] == 'TARGET']\n",
    "#export = tgt.groupby(by=['MaterialNumber'],as_index=False).sum().sort_values(by=['loss'],ascending=False)\n",
    "#export.to_excel('tgt cuts_all_items_c.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_items_nc['Cuts'] = [y if x > y else x for x,y in zip(by_items_nc['Cuts'],by_items_nc['QuantityExpected'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study.name = 'study'\n",
    "study_nocap.name = 'study_nocap'\n",
    "#by_customer.name = 'by_customer'\n",
    "by_customer_nc.name = 'by_customer_nc'\n",
    "by_items_nc.name = 'by_items_nc'\n",
    "#by_items.name = 'by_items'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_mod = []\n",
    "        \n",
    "date_mod_nc = []\n",
    "for x in list(study_nocap['date']):\n",
    "    if x != 0:\n",
    "        temp_date_mod_nc = []\n",
    "        for y in x:\n",
    "            try:\n",
    "                z = y.strftime(\"%m/%d/%Y\")\n",
    "                temp_date_mod_nc += [z]\n",
    "            except:\n",
    "                pass\n",
    "        date_mod_nc += [temp_date_mod_nc]\n",
    "    else:\n",
    "        date_mod_nc += ['not run']\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study['date_mod'] = date_mod\n",
    "study_nocap['date_mod'] = date_mod_nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep for full export:\n",
    "df_f_m = df_f_m.reset_index(drop=True)\n",
    "\n",
    "#df_list = [study,study_nocap,by_customer,by_customer_nc,by_items_nc,by_items]\n",
    "df_list = [study_nocap,by_customer_nc,by_items_nc]\n",
    "\n",
    "#study.name = 'study'\n",
    "study_nocap.name = 'study_1.0'\n",
    "#by_customer.name = 'by_customer'\n",
    "by_customer_nc.name = 'by_customer_1.0'\n",
    "by_items_nc.name = 'by_items_1.0'\n",
    "#by_items.name = 'by_items'\n",
    "\n",
    "writer = ExcelWriter('Repeated_Ordering_1.0_2_22_2020.xlsx')\n",
    "for n, df in enumerate(df_list):\n",
    "    df.to_excel(writer, str(df.name))\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study_nocap.to_excel('WMT_cut_fake_cuts.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
